# Case t√©cnico para a vaga para Analista de Dados Jr (python) na Join üíª

‚ö†Ô∏è OBS: Os arquivos gerados das tabelas nos formatos parquet e os arquivos das respostas em formato delta v√£o ser salvos em seu 
dbs/filestore do databricks. Como utilizei o databricks e minha conte n√£o √© premium, n√£o conseguir configurar o CLI para baixar os arquivos gerados parquet e delta
e tamb√©m n√£o tenho acesso a armazenamento em nuvem para passar os arquivos para nuvem para baixar. 
Por√©m, quando seguir as etapas, esses arquivos ser√£o salvos no seu dbfs:filestore e voc√™ ter√° acesso, e conseguir√° compilar:
Segue as etapas abaixo para a aplica√ß√£o. 

üìã Este reposit√≥rio foi criado para armazena os codigos desenvolvidos para o case t√©cnico. 

O que cont√©m no reposit√≥rio quais tarefas s√£o realizadas. ‚öíÔ∏è

Este case utiliza um banco de dados na nuv√©m chamado Ecommerce website database, que pode ser acessado no link: https://uibakery.io/sql-playground. 
A parti desse banco de dados foi criado um DER utilizando uma ferramenta gratuita que se chama br_modelo para fazer o diagrama entidade e relacionamento, foi feito o diagrama logico tamb√©m.  

* O DER esta salvo dentro de uma pasta do reposit√≥rio  chamada DER. Nesta pasta tem 4 arquivos. 

* DER.brM3, DER_ecommerce_logico.brM3  ambos foram criados na ferramenta chamada br_modelo. Ap√≥s a cria√ß√£o dos DER, foi exportado em formato png que s√£o cada arquivo correspondente ao .brM3 que se s√£o:
* DER.png e DER_ecommerce_logico.png. Realizei a exporta√ß√£o nesse formato, para que n√£o seja necess√°ro baixar o br_modelo que √© um execut√°vel para abrir os DER. Neste caso pode apenas abrir os dois arquivos com extensao png e observa os dois DER.png o DER e o DER_ecommerce_logico.png em seu computador. 

* O reposit√≥rio tamb√©m  cont√©m um  arquivo chamado desafio_join_notebook.ipynb üíªüìãüìéüìÅ

* O que esse arquivo faz: desafio_join_notebook.ipynb 
  neste notebook criado no Databricks ele cont√©m c√≥digos  para realizar a conex√£o com um banco de dados da nuv√©m citados acima. Com a utiliza√ß√£o do driver JDBC foi realizado a conex√£o com o banco de dados. Ap√≥s fazer a conex√£o, foi realizado a etapa de codifica√ß√£o para extrair dados das tabelas dentro do notebook que cont√©m os c√≥digos. 

* Na etapa de extra√ß√£o das tabelas, foi utilizado 2 fun√ß√µes. Uma para fazer a conex√£o com o banco de dados chamada: conexao, que foi respons√°vel pela conex√£o com o banco de dados acessando suas credenciais e pecorrendo as tabelas existentes, para a extra√ß√£o e fazer a chamada da fun√ß√£o: mostrar_tabelas_extrair passando as colunas existentes como parametro e as credencias do banco.
* A outra fun√ß√£o chamada: mostrar_tabelas_extrair √© responsavel por receber as credenciis da fun√ß√£o anterior, receber as tabelas pecorridas e salvar elas utilizando o pyspark read.  O salvamento delas foi em formato parquet. 
* Para realizar a chamada dessas fun√ß√µes foi utilizado if __name__=="__main__": que √© respons√°vel para realizar a chamar da fun√ß√£o: conexao.
#### Etapa Seguintes üìã
* Ap√≥s o salvamento das tabelas em formato parquet foi iniciado o processo de consulta de informa√ß√µes solicitadas a respeito das colunas. Foram:
- Qual pa√≠s possui a maior quantidade de itens cancelados?
- Qual o faturamento da linha de produto mais vendido, considere como os itens Shipped, cujo o pedido foi realizado no ano de 2005?
- Nome, sobrenome e e-mail dos vendedores do Jap√£o, o local-part do e-mail deve estar mascarado.
* utilizando o pyspark com sql, foi realizada as consultas para extrair as respostas das perguntas acima. Com pyspark read foi realizado a abertura dos aquivos parquet, apos abrir os arquivos foi criado  utilizando a fun√ß√£o createOrReplaceTempView no pyspark para transformar cada arquivo aberto no formato parquet, em um template tempor√°rio. 
* Ap√≥s essa transforma√ß√£o, foi realizar as consultas sql para responder as perguntas, e salvar as consultas em formato delta utilizando pyspark write. 
* As consultas foram salvas em formato delta e tamb√©m exibidas na tela do notebook com a fun√ß√£o display do pyspark. Podendo assim, ver o resultado das informa√ß√µes das respostas das perguntas. 
#### Como rodar os c√≥digos ‚öíÔ∏è
* 1-Dentro do projeto no github clone o reposit√≥rio. 
* 2-Ap√≥s fazer o clone, utilize notebook: desafio_join_notebook.ipynb que cont√©m os codigos dos procedimentos explicados neste README e importe o  notebook desafio_join_notebook.ipynb para o databricks.  
* 3-Ap√≥s isso, compile os codigos do notebook e aguarde at√© o fim do processamento.
* 4- Voc√™ ter√° as tabelas extraidas no formato parquet salvas no seu dbfs:file/store do databricks e tamb√©m as consultas das respostas salvas no formato delta. 

Ferramentas e linguagens utilizadas para o desenvolvimento.
* ![Python](https://img.shields.io/badge/-Python-yellow?style=for-the-badge&logo=Python)
* ![Databricks](https://img.shields.io/badge/-databricks-blue?style=for-the-badge&logo=databricks)
* ![Pandas](https://img.shields.io/badge/-Pandas-222222?style=for-the-badge&logo=Pandas)
* ![PySpark](https://img.shields.io/badge/-PySpark-green?style=for-the-badge&logo=PySpark)
